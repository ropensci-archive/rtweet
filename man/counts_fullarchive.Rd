% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/premium.R
\name{counts_fullarchive}
\alias{counts_fullarchive}
\alias{counts_30day}
\title{Premium Twitter searches}
\usage{
counts_fullarchive(
  q,
  n = 100,
  fromDate = NULL,
  toDate = NULL,
  continue = NULL,
  bucket = NULL,
  env_name = NULL,
  premium = FALSE,
  safedir = NULL,
  parse = TRUE,
  token = NULL
)

counts_30day(
  q,
  n = 100,
  fromDate = NULL,
  toDate = NULL,
  env_name = NULL,
  continue = NULL,
  bucket = NULL,
  premium = FALSE,
  safedir = NULL,
  parse = TRUE,
  token = NULL
)
}
\arguments{
\item{q}{Search query on which to match/filter tweets. See details for
information about available search operators.}

\item{n}{Desired number of results to return. Results are downloaded
in pages when \code{n} is large; the default value will download a single
page. Set \code{n = Inf} to download as many results as possible.

The Twitter API rate limits the number of requests you can perform
in each 15 minute period. The easiest way to download more than that is
to use \code{retryonratelimit = TRUE}.

You are not guaranteed to get exactly \code{n} results back. You will get
fewer results when tweets have been deleted or if you hit a rate limit.
You will get more results if you ask for a number of tweets that's not
a multiple of page size, e.g. if you request \code{n = 150} and the page
size is 200, you'll get 200 results back.}

\item{fromDate}{Oldest date-time (YYYYMMDDHHMM) from which tweets should be
searched for.}

\item{toDate}{Newest date-time (YYYYMMDDHHMM) from which tweets should be
searched for.}

\item{continue}{A character string with the next results of a query. You
must make the exact same query as the original, including \code{q}, \code{toDate},
\code{fromDate} and \code{bucket}.}

\item{bucket}{Unit of time the count data will be provided. Options, one of:
"day", "hour", "minute".}

\item{env_name}{Name/label of developer environment to use for the search.}

\item{premium}{A logical value if the environment is paid (TRUE) or
sandboxed, the default (FALSE). It limits the number of results retrieved so the number
of API queries needed to retrieve \code{n} results.}

\item{safedir}{Name of directory to which each response object should be
saved. If the directory doesn't exist, it will be created. If NULL (the
default) then a dir will be created in the current working directory. To
override/deactivate safedir set this to FALSE.}

\item{parse}{If \code{TRUE}, the default, returns a tidy data frame. Use \code{FALSE}
to return the "raw" list corresponding to the JSON returned from the
Twitter API.}

\item{token}{Expert use only. Use this to override authentication for
a single API call. In most cases you are better off changing the
default for all calls. See \code{\link[=auth_as]{auth_as()}} for details.}
}
\value{
A tibble data frame of Twitter data for each unit of bucket.
Counts are only an estimate. It shouldn't be expected that the count your
receive will be the exact number of activities returned. However, you can
and should expect that counts will always return a higher value than the
number of activities returned via the data endpoints.
}
\description{
Count tweets on the 30day or fullarchive premium products.
}
\details{
Note: The \code{env_name} must match the ones you set up for the token you are using.
}
\examples{

\dontrun{
## search fullarchive for up to 300 rstats tweets sent in Jan 2014
rt <- count_fullarchive("#rstats", n = 300, env_name = "SetYourLabel",
  fromDate = "201401010000", toDate = "201401312359")

toDate <- format(Sys.time() - 60 * 60 * 24 * 7, "\%Y\%m\%d\%H\%M")

## search 30day for up to 300 rstats tweets sent before the last week
rt <- count_30day("#rstats", n = 300,
  env_name = "SetYourLabel", toDate = toDate)
}

}
\references{
\url{https://developer.twitter.com/en/docs/twitter-api/premium/search-api/api-reference/premium-search#CountsEndpoint}
}
\seealso{
Other premium endpoints: 
\code{\link{search_fullarchive}()}
}
\concept{premium endpoints}
